# ğŸ› ï¸ Analyser & InterprÃ©ter les RÃ©sultats du Scan â€“ SonarQube (Dragon Project)

## ğŸ“Š Project Overview â€“ SonarQube Dashboard

![Dashboard Overview](screenshots/dashboard-overview.png)

| Metric                         | Result                | Rating | Interpretation                         |
| ------------------------------ | --------------------- | ------ | -------------------------------------- |
| **Bugs**                       | 23                    | C      | Medium reliability risk                |
| **Vulnerabilities**            | 0                     | A      | No verified security flaws             |
| **Security Hotspots Reviewed** | 0%                    | E      | No security review done (critical)     |
| **Code Smells**                | 1.4k                  | A      | Maintainability issues but no blockers |
| **Coverage**                   | 0%                    | â€”      | No unit tests detected                 |
| **Duplications**               | 17.2%                 | â€”      | High code duplication                  |
| **Lines of Code**              | ~15k                  | â€”      | Project size                           |
| **Languages**                  | Python, HTML, JS, CSS | â€”      |                                        |

## ğŸ“˜ SonarQube Quality Profile â€“ What Rules Are Applied to the Dragon Project?
Every SonarQube analysis is driven by a Quality Profile.

A Quality Profile is essentially the **Rulebook** that tells SonarQube:

- which rules should be applied
- how strict the analysis should be
- which severities rules have
- which patterns are considered bugs, security issues, hotspots, or smells

Think of it as the **linting + security policy** that defines how your code is evaluated.
![Profile Quanlity HTML](screenshots/profileQuanlity.png)

### ğŸ” 1. Which Quality Profile Was Applied to the Dragon Project?

For this scan, the project used the default SonarQube Built-in Quality Profiles:

| Language       | Quality Profile Applied                       |
| -------------- | --------------------------------------------- |
| **Python**     | Sonar way                                     |
| **JavaScript** | Sonar way                                     |
| **HTML**       | Sonar way (includes WCAG accessibility rules) |
| **CSS**        | Sonar way                                     |


These profiles include hundreds of rules:

| Language | Rule Set                                    | Purpose                                |
| -------- | ------------------------------------------- | -------------------------------------- |
| Python   | Python Sonar way                            | Bugs, security, maintainability        |
| HTML     | HTML Sonar way (WCAG + accessibility rules) | Accessibility, semantics, structure    |
| JS       | JavaScript Sonar way                        | Standard JS coding conventions         |
| CSS      | CSS Sonar way                               | Style cleanliness + dead rules cleanup |
 
> Each rule is categorized as Bug, Vulnerability, Security Hotspot, or Code Smell, and has a severity level (Blocker, Critical, Major, Minor, Info).
![Profile Quanlity Dragon](screenshots/profileQuanlityDrg.png)

### ğŸ§  2. Why It Matters

The Quality Profile determines what SonarQube detects.

Example:

- HTML Minor issues â†’ triggered by WCAG accessibility rules (missing lang, table captions, ARIA labelsâ€¦)
- Python code smells â†’ triggered by maintainability rules (complex functions, duplicated logic)
- Vulnerabilities â†’ depend on the security rules enabled in the Python or JS profiles

Essentially, changing the profile changes the analysis results, so understanding which profile is applied is critical.

### ğŸ” 3. Example Rules Coming from the Quality Profile

#### HTML (Accessibility â€“ WCAG AA)

![Profile Quanlity HTML](screenshots/profileQuanlityHTML.png)

- ```bash <html> ``` must have a lang attribute
- ```bash <table> ``` must have a ```bash <caption> or <summary> ```
- ```bash <img> requires alt text ```
- Input labels must be associated with controls

â†’ These rules caused most of the Minor and Info issues.

#### Python (Maintainability + Security)
![Profile Quanlity Python](screenshots/profileQuanlityPython.png)

- Always-false conditions
- Conditional branches with same code
- Duplicate logic
- Hard-coded credentials (Security Hotspots)
- Complex functions too long or too nested
- Duplicated code blocks

![Profile Quanlity Python rules](screenshots/profileQuanlityPythonRules.png)

â†’ These rules generated most Major code issues and  code smells.

### 4. Can You Change the Quality Profile?

Yes. SonarQube provides full control over rules and profiles.

| Action                     | Allowed? | Explanation                                                  |
| -------------------------- | -------- | ------------------------------------------------------------ |
| Assign a different profile | âœ… Yes    | You can assign a profile per language or per project         |
| Create a custom profile    | âœ… Yes    | Extend a built-in profile (â€œSonar wayâ€) and add/remove rules |
| Add new rules              | âœ… Yes    | Via plugins or custom regex-based rules                      |
| Disable rules              | âœ… Yes    | If they donâ€™t apply to your project                          |
| Change severity of rules   | âš ï¸ Yes   | Only for custom or non-SonarSource rules                     |

**Example custom profiles you could create:**

* **Python Security Extended** â†’ extra security rules for Python
* **React Strict TS Rules** â†’ stricter TypeScript rules for React components
* **WCAG AAA HTML Profile** â†’ higher-level accessibility compliance for HTML

---

### ğŸ“ 5. How SonarQube Chooses the Profile for Each File

* **By Language**: SonarQube automatically applies the profile associated with the fileâ€™s language.
* **Per-Project Override**: You can assign specific profiles to your project instead of the default.
* **Per-File Override**: With **project-level settings or file path filters**, you can assign a different profile for specific files.

> This means you could have **Python files using â€œSonar wayâ€**, but specific critical modules could use **â€œPython Security Extendedâ€**, and HTML templates could follow a stricter WCAG profile.


# â­ Understanding Severity Levels (Blocker â†’ Info)
SonarQube assigns every issue a Severity.
Severity describes how serious the problem is and how urgent the fix should be.

There are 5 severity levels, from most important to least important:
# ğŸ”¥ 1. Blocker â€” Must be fixed immediately

### Meaning:
A blocker issue represents a critical defect or security flaw that will break the application, cause data corruption, crashes, or expose a dangerous vulnerability.

### Examples:
- null pointer exceptions
- infinite loops
- SQL injection detected
- fatal configuration issues
***Action***: Fix now â€” project is not stable until resolved.

# ğŸš¨ 2. Critical â€” Fix as soon as possible
### Meaning:
High-risk issues that donâ€™t break the system immediately but create major security or reliability risks.

### Examples:
- insecure authentication logic
- unsafe cryptography
- dangerous resource handling
- always-false conditions in security code
***Action***: Fix ASAP â€” next sprint at the latest.

# âš ï¸ 3. Major â€” Fix soon (within sprint)

### Meaning:
Important maintainability or logic issues that make the code fragile, error-prone, or misleading.

### Examples:
- incorrect conditions
- dead code
- complex logic
- missing required HTML attributes
***Action***: Fix during normal development cycles.

# ğŸŸ¡ 4. Minor â€” Low priority improvements

### Meaning:
Small quality improvements that improve clarity but do not affect behavior.

### Examples:
- naming improvements
- optional refactors
- readability enhancements
***Action***: Fix when touching the file (boy-scout rule).

# ğŸ”µ 5. Info â€” Informational only

### Meaning:
Suggestions or optional improvements with no impact on correctness.

### Examples:
- optional comments
- best practice recommendations
***Action***: Optional.

# âœ” How This Applies to Dragon

From the scan:
| Severity | Bugs | Interpretation                 |
| -------- | ---- | ------------------------------ |
| Blocker  | 0    | No breaking issues ğŸ‘          |
| Critical | 0    | Good security state in bugs ğŸ‘ |
| Major    | 3    | Must be fixed (logic issues)   |
| Minor    | 20   | Mostly HTML accessibility      |
| Info     | 0â€“1  | Optional only                  |

### Interpretation

- No Critical or Blocker bugs â†’ Good
- 3 Major bugs â†’ Need attention (can break logic flow)
- 20 Minor bugs â†’ Cosmetic, accessibility-related, mostly in HTML
- Majority of bugs come from HTML accessibility rules (WCAG)
    - <table> elements missing descriptions
    - <html> missing lang attribute

# 1. ğŸ” How to Analyze & Interpret the Results

## 1.1 ğŸ Bugs (23 â€“ Rating C)

![Bugs](screenshots/bugs.png)

### â¤ What it means
Bugs represent coding errors that may cause:
- crashes
- incorrect outputs
- incorrect logic flow

**Rating C = acceptable but requires improvements.**

### ğŸ§© What to do
- Fix **Critical** and **Major** bugs first.
- Minor bugs â†’ backlog.

### Example Bug (Real case from Dragon project)

![Bug Example](screenshots/bug-example.png)

### Issue summary

Rule: python:S3923 â€“ Conditional branches should not have the same implementation
File: backend/app/services/application/model_resolver.py
Severity: Major
Effort: 15min
Status: Open

### âŒ The Problematic Code
```bash
impl = self._llm_cfg.get("impl")
provider = self._llm_cfg.get("provider")
model = self._llm_cfg.get("model")
if impl is None:
    impl = "llm.openai" if provider == "openai" else "llm.openai"
# This always returns "llm.openai"
```

### âš  Why this is a Bug
![Why Example](screenshots/why-bug-example.png)

The conditional expression:

```bash
"llm.openai" if provider == "openai" else "llm.openai"
```

returns the same value in both branches, making the condition meaningless.

### âœ” How to Fix it

If a default is needed:

```bash
impl = impl or "llm.openai"
```

## 1.2 ğŸ›¡ï¸ Vulnerabilities (0 â€“ Rating A)

![Vulnerabilities](screenshots/vulnerabilities.png)

### ğŸ” What it means
No confirmed vulnerabilities detected by SonarQube.

### ğŸ‘ Interpretation
Very good state.

âš ï¸ *But final security depends heavily on reviewing Security Hotspots.*

## 1.3 ğŸš¨ Security Hotspots (0% Reviewed â€“ Rating E)

![Security Hotspots](screenshots/hotspots.png)

### ğŸ” What it means
Hotspots = sensitive code patterns requiring **manual security review**.
Rating **E** = none were reviewed â†’ **critical security risk**.

### â— Why it matters
Even without vulnerabilities, hotspots may hide:
- unsafe API usage
- insecure file operations
- unvalidated inputs
- unsafe Python/Javascript patterns

### ğŸ§© What to do
Review each hotspot and classify as:
- **Reviewed**
- **Fixed**
- **Safe**

### Example Security Hotspots (Real case from Dragon project)

![Hotspot Example](screenshots/hotspot-example.png)

### ğŸ”¥ Example Hotspot

Rule: python:S2068
Risk: Hard-coded credentials detected
File: backend/tests/test_auth_api.py
Status: To review
Review Priority: High
Severity Type: Authentication

### ğŸ” Where is the risk?

In the code:
```bash
payload = {
    "username": username,
    "email": f"{username}@example.com",
    "password": "AhmedAkalatofa7a123!",
}
```
SonarQube detects "password" + literal string â†’ potential hard-coded credential.

Even if it's in tests, it must be reviewed.

### âš  Why is this a Hotspot?
![Hotspot Example](screenshots/what-hotspot-example.png)

Hard-coded credentials can lead to:

- accidental leakage
- credential reuse by developers
- secrets ending in git history
- test code exposing sensitive data

Sonar does not assume itâ€™s dangerous â†’
YOU must validate manually.


### âœ” How to assess the risk
![Hotspot Example](screenshots/assess-hotspot-example.png)

Do the following check:

- Is this credential used in production?
- Is the password recycled in configs?
- Is this OK because itâ€™s fake test data?
- Does it expose any secrets outside test scope?

If safe â†’ mark as Reviewed.

## âœ” How to fix
![Hotspot Example](screenshots/how-hotspot-example.png)

```bash

Replace by environment-based config:
    "password": os.getenv("TEST_USER_PASSWORD", "dummy-password"),
OR
    Use a factory/mocking system instead of explicit strings.
```

## 1.4 ğŸ§¹ Code Smells (1.4k â€“ Rating A)

![Code Smells](screenshots/code-smells.png)

### ğŸ” What it means
Maintainability issues such as:
- duplicated logic
- long functions
- complex conditions
- poor naming

Rating A = **Good maintainability**, but **1.4k smells** still require cleanup.

### ğŸ§© What to do
- Prioritize **Major** & **Critical** smells
- Minor smells â†’ backlog refactoring

### Example Code Smell (Real case from Dragon project)
![Code Smells example](screenshots/example-code-smells.png)

### ğŸ“ Issue Summary

Rule: python:S5727
Title: Remove this identity check; it will always be False.
File: backend/app/api/dependencies.py
Severity: Critical
Effort: 10 min
Status: Open

### âŒ Problematic Code

```bash
    payload = verify_token(token, token_type="access")
    if not payload:
        raise credentials_exception
    sub = str(payload.get("sub"))

    if sub is None:
        # âŒ This condition will ALWAYS be False
        raise credentials_exception

    user = get_user_by_sub(db, sub)
    if user is None:
        raise credentials_exception
    return user
```

### ğŸ” Where is the Issue?
This line is the problem:

```bash
if sub is None:
```

Because just before, we did:

```bash
sub = str(payload.get("sub"))
```

Converting a value to str() never returns None.
Even if payload.get("sub") is None â†’ str(None) becomes:

```bash
"None"   # a string, not None
```
So the condition will never trigger.
Sonar flags this as:

    â€œRemove this identity check; it will always be False.â€

### âš  Why is this an Issue?
![Code Smells example](screenshots/why-exemple-code-smells-1.png)
![Code Smells example](screenshots/why-exemple-code-smells-2.png)


## 1.5 ğŸ§ª Coverage â€“ 0%
![Coverage](screenshots/coverage1.png)
![Coverage](screenshots/coverage.png)

### ğŸ” What it means
Coverage shows how much of your source code is executed by automated tests.
SonarQube does not run tests itself â€” it only reads coverage reports generated by external tools (pytest, Jest, etc.).

### â— Why Coverage Was 0% (Even With Unit Tests)

When the first scan was executed with:
```bash
docker run --rm --network host -v "$(pwd):/usr/src" sonarsource/sonar-scanner-cli \
  -Dsonar.projectKey=dragon-develop5 \
  -Dsonar.sources=. \
  -Dsonar.host.url=http://localhost:9000
```
SonarQube detected:
- No coverage report
- No test execution results

Therefore:
ğŸ“‰ Coverage = 0%
This is expected behavior because SonarQube only interprets an existing XML report â€” it does not generate one.

###  2. Step 1 â€” Generate Coverage Report Using Pytest

```bash
cd backend
poetry run pytest --cov=. --cov-report=xml:coverage.xml
```

This generates:
```bash
backend/coverage.xml
```
SonarQube needs this XML file to calculate coverage.

### 3. Step 2 â€” Fix Coverage Paths When Scanning From Docker

When scanning via Docker, the project is mounted under:
```bash
/usr/src
```
But the generated coverage.xml contained local filesystem paths such as:

```bash
/Users/you/dragon/backend/app
```
Since these paths do not exist inside the container, SonarQube could not match them to your code â†’ coverage ignored.
Manual Fix Applied

The <source> paths inside coverage.xml were updated to match Dockerâ€™s mounted directory:

<source>/usr/src/backend</source>
<source>/usr/src/backend/app</source>
This ensures SonarQube can correctly map the coverage data to scanned files.

![Coverage](screenshots/coverageXML.png)

### 4. Step 3 â€” Run SonarQube Scan With the Coverage Report Enabled

The scan command was updated to include the coverage path:
```bash
-Dsonar.python.coverage.reportPaths=backend/coverage.xml
```
new working scan command:

docker run --rm --network host -v "$(pwd):/usr/src" sonarsource/sonar-scanner-cli \
  -Dsonar.projectKey=dragon-develop5 \
  -Dsonar.projectName=dragon-develop5 \
  -Dsonar.sources=. \
  -Dsonar.host.url=http://localhost:9000 \
  -Dsonar.login=admin \
  -Dsonar.password=@dmin \
  -Dsonar.python.version=3 \
  -Dsonar.python.coverage.reportPaths=backend/coverage.xml

ğŸ“ˆ 5. Result After Integrating Coverage Properly

After fixing the paths and re-running the scan:

- Coverage is correctly detected
- Test execution is visible inside SonarQube
- Dashboard now reflects real coverage percentages


![Coverage](screenshots/coverage-after-fix.png)
![Coverage](screenshots/coverage-after-fix1.png)


## 1.6 ğŸ” Code Duplications â€“ 17.2%

![Duplications](screenshots/duplications.png)

### ğŸ” What it means

Code duplication occurs when identical or very similar blocks of code appear multiple times across the project.
High duplication increases maintenance effort and the risk of introducing bugs when changes are made.

### ğŸ“Š Key Metrics

| Metric                | Value | Explanation                                                |
| --------------------- | ----- | ---------------------------------------------------------- |
| **Density**           | 17.2% |The percentage of your total code that is duplicated        | 
| **Duplicated Lines**  | 4,617 |Total number of lines that are duplicated across the project|
| **Duplicated Blocks** | 28    |A block is a chunk of consecutive duplicated lines          |
| **Duplicated Files**  | 4     |Number of files that contain duplicated code                |

### â— Impact

* Higher risk of bugs due to repeated logic
* Harder to maintain and refactor
* Violates clean coding principles and DRY (Donâ€™t Repeat Yourself) rules

### ğŸ§© Recommended Actions

* Refactor repeated code into **shared modules** or **utility functions**
* Create **reusable components** for duplicated logic
* Consolidate similar code blocks to improve maintainability
* Monitor duplication density after each major refactor


# 2. ğŸ¯ Sonar Ratings Explained (A â†’ E)

![Ratings](screenshots/ratings-legend.png)

| Rating | Meaning    | Interpretation                |
| ------ | ---------- | ----------------------------- |
| **A**  | Excellent  | No significant issues         |
| **B**  | Good       | Minor improvements needed     |
| **C**  | Acceptable | Moderate issues               |
| **D**  | Poor       | Requires urgent fixes         |
| **E**  | Critical   | Severe issues â€“ must be fixed |

# 3. ğŸ§  Apply to the Dragon Project

| Category        | Rating | Meaning                   |
| --------------- | ------ | ------------------------- |
| Vulnerabilities | **A**  | Secure                    |
| Code Smells     | **A**  | Maintainable              |
| Bugs            | **C**  | Needs improvements        |
| Hotspot Review  | **E**  | âš ï¸ Critical security risk |

# 4. ğŸ“‰ Technical Debt

![Technical Debt](screenshots/technical-debt.png)

Sonar estimates the required time to fix maintainability issues.

### âœï¸ Interpretation
- High number of code smells â†’ higher technical debt
- Rating A â†’ the ratio is still acceptable

### ğŸ§© What to do
- Reduce duplication
- Fix complex code
- Simplify long functions and nested conditions

# 5. ğŸ§­ Priority Action Plan

## ğŸ”¥ High Priority (Fix immediately)
- Review **ALL Security Hotspots** (Rating E)
- Add **unit tests** (0% coverage)
- Fix **critical & major bugs**
- Reduce **critical duplicated blocks**

## ğŸŸ¡ Medium Priority
- Refactor files with many code smells
- Simplify complex methods

## ğŸŸ¢ Low Priority
- UI / formatting improvements
- Minor code smells
